{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADH (2013) with composition adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "#from linearmodels.iv import IV2SLS, compare\n",
    "from io import StringIO\n",
    "import warnings\n",
    "import numpy as np\n",
    "from econtools import group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weighted aggregation\n",
    "\n",
    "def WtSum(df:pd.core.frame.DataFrame, cols:list, weight_col:str, by_cols:list, \n",
    "          outw=False, mask=None):\n",
    "    '''Weighted sum'''\n",
    "    \n",
    "    out = df[[*cols, weight_col, *by_cols]].copy()\n",
    "    out[[*cols, weight_col]] = out[[*cols, weight_col]].astype(np.float64) #for sum precision\n",
    "    \n",
    "    if mask is not None:\n",
    "        out = out[mask]\n",
    "\n",
    "    for c in cols:\n",
    "        out[c] = out[c] * out[weight_col]\n",
    "    \n",
    "    if outw:\n",
    "        return out.groupby(by_cols)[[*cols, weight_col]].sum()\n",
    "    else:\n",
    "        return out.groupby(by_cols)[cols].sum()\n",
    "\n",
    "def WtMean(df:pd.core.frame.DataFrame, cols:list, weight_col:str, by_cols:list, \n",
    "           mask=None):\n",
    "    '''Weighted mean'''    \n",
    "    \n",
    "    out_list = []\n",
    "    for c in cols:\n",
    "        out = df[[c, weight_col, *by_cols]].copy()\n",
    "        out[[c, weight_col]] = out[[c, weight_col]].astype(np.float64) #for sum precision\n",
    "\n",
    "        if mask is not None:\n",
    "            out = out[mask]\n",
    "            \n",
    "        out = out[~np.isnan(out[c])] #remove missings\n",
    "        out.loc[:,c] = out.loc[:,c] * out.loc[:,weight_col] #multiply by weights\n",
    "        out = out.groupby(by_cols)[[c, weight_col]].sum() #sum\n",
    "        out.loc[:,c] = out.loc[:,c] / out.loc[:,weight_col] # divide by total weights\n",
    "\n",
    "        out_list.append(out[c])\n",
    "\n",
    "    return pd.concat(out_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Composition-Adjusted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column definitions:\n",
    "pd.read_stata('usa_00137.dta', iterator=True).variable_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('usa_00137.dta', convert_categoricals=False)\n",
    "# Keep those aged 16-64 and not in group quarters:\n",
    "df = df[(df.age>=16) & (df.age<=64) & (df.gq<=2)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define (128) groups over which we CA: gender (2) x US born (2) x age bin (4) x education bin (4) x race bin (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agebin'] = pd.cut(df.age, bins=[15,27,39,51,64], labels=False)\n",
    "df['educbin'] = pd.cut(df.educ, bins=[-1,5,6,9,11], labels=False)\n",
    "df['college'] = np.where((df.educ>9) & (df.educ<=11), 1, 0)\n",
    "df['white'] = np.where(df.race==1, 1, 0)\n",
    "df['native'] = np.where(df.bpl<=99, 1, 0)\n",
    "df['male'] = np.where(df.sex==1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['age','educ','race','bpl','sex'], inplace=True)\n",
    "group_cols = ['male', 'native', 'agebin', 'educbin', 'white']\n",
    "df = group_id(df, cols=group_cols, merge=True, name='groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get geography to cz level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.statefip==22)&(df.puma==77777), 'puma'] = 1801 #Katrina data issue\n",
    "df['PUMA'] = df['statefip'].astype(str).str.zfill(2) + df['puma'].astype(str).str.zfill(4)\n",
    "df['PUMA'] = df['PUMA'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1990 = df[df.year==1990].merge(pd.read_stata(mainp / 'files_provided/cw_puma1990_czone.dta'),\n",
    "                                 left_on='PUMA', right_on='puma1990')\n",
    "df2000 = df[df.year!=1990].merge(pd.read_stata(mainp / 'files_provided/cw_puma2000_czone.dta'),\n",
    "                                 left_on='PUMA', right_on='puma2000')\n",
    "df = pd.concat([df1990, df2000])\n",
    "df['perwt'] = df['perwt'] * df['afactor']\n",
    "del df1990; del df2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate to cz x group x year level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment status:\n",
    "df['emp'] = np.where(df.empstat==1, 1, 0)\n",
    "df['unemp'] = np.where(df.empstat==2, 1, 0)\n",
    "df['nilf'] = np.where(df.empstat==3, 1, 0)\n",
    "# Manufacturing employment:\n",
    "df['manuf'] = np.where((df.emp==1) & (df.ind1990>=100) & (df.ind1990<400), 1, 0)\n",
    "df['nonmanuf'] = np.where((df.emp==1) & ((df.ind1990<100) | (df.ind1990>=400)), 1, 0)\n",
    "# Filling in weeks worked for 2008 ACS (using midpoint):\n",
    "df.loc[df.wkswork2==1, 'wkswork1'] = 7\n",
    "df.loc[df.wkswork2==2, 'wkswork1'] = 20\n",
    "df.loc[df.wkswork2==3, 'wkswork1'] = 33\n",
    "df.loc[df.wkswork2==4, 'wkswork1'] = 43.5\n",
    "df.loc[df.wkswork2==5, 'wkswork1'] = 48.5\n",
    "df.loc[df.wkswork2==6, 'wkswork1'] = 51\n",
    "# Log weekly wage:\n",
    "df['lnwkwage'] = np.log(df.incwage/df.wkswork1)\n",
    "df.loc[df['lnwkwage']==-np.inf, 'lnwkwage'] = np.nan\n",
    "# Hours:\n",
    "df['hours'] = df['uhrswork'] * df['wkswork1']\n",
    "\n",
    "df.drop(columns=['empstat','wkswork2','incwage'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmean_cols = ['lnwkwage']                                    #columns to take weighted mean\n",
    "sum_cols = ['manuf','nonmanuf','emp','unemp','nilf','hours'] #columns to sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_cols=['czone','year','groups',*group_cols,'college']\n",
    "df_cgy = pd.concat(\n",
    "    [WtMean(df, cols=wmean_cols, weight_col='perwt', by_cols=by_cols),\n",
    "     WtSum(df, cols=sum_cols, weight_col='perwt', by_cols=by_cols, outw=True)]\n",
    "    , axis=1\n",
    ")\n",
    "df_cgy.rename(columns={'perwt':'pop'}, inplace=True)\n",
    "\n",
    "for c in ['manuf','nonmanuf','unemp','nilf']:\n",
    "    df_cgy['{}_share'.format(c)] = df_cgy[c] / df_cgy['pop']\n",
    "\n",
    "for c in [*sum_cols,'pop']:\n",
    "    df_cgy['ln{}'.format(c)] = np.log(df_cgy[c])\n",
    "    df_cgy.loc[df_cgy['ln{}'.format(c)]==-np.inf, 'ln{}'.format(c)] = np.nan\n",
    "    \n",
    "del df\n",
    "df_cgy = df_cgy.reset_index().set_index(['czone','year','groups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cgy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate to cz x year level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a database at the level of the commuting zone ($i$) by year ($t$) by group ($g$). For the regressions we need data at the level of commuting zone by year ($it$). We will construct composition-adjusted measures as\n",
    "\n",
    "$$L_{it}^{CA} = \\sum_g \\bar{\\theta}_{ig} L_{igt}$$\n",
    "\n",
    "where the time-invariant weights $\\bar{\\theta}_{ig}$ are the average across periods of hours weights:\n",
    "\n",
    "$$\n",
    "\\bar{\\theta}_{ig} = \\frac{1}{3} \\left( \\theta_{ig1990}+ \\theta_{ig2000}+ \\theta_{ig2008}\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta_{igt} = hours_{igt} \\Big/ \\left( \\sum_g hours_{igt} \\right).\n",
    "$$\n",
    "\n",
    "    \n",
    "Note that $\\sum_g \\bar{\\theta}_{ig}=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights\n",
    "df_w = df_cgy.reset_index()[['czone','year','groups','hours']].copy()\n",
    "\n",
    "# Deal with missing obs as zeros (which they are):\n",
    "df_w = df_w.set_index(['czone','year','groups']).unstack(level=[1,2], fill_value=0.0).stack(level=[1,2])\n",
    "\n",
    "df_w['weight_cgt'] = df_w['hours'] / df_w.groupby(['czone','year'])['hours'].transform('sum')\n",
    "df_w['weight_cg'] = df_w.groupby(['czone','groups'])['weight_cgt'].transform('mean')\n",
    "\n",
    "df_cgy = pd.concat([df_cgy, \n",
    "                    df_w[['weight_cg']].rename(columns={'weight_cg':'weight'})\n",
    "                   ], axis=1)\n",
    "\n",
    "del df_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the average log wages across various aggregations within a czone x year\n",
    "def fun(m): \n",
    "    return WtMean(df_cgy.reset_index(), cols=['lnwkwage'], \n",
    "                  weight_col='weight', by_cols=['czone','year'], mask=m)\n",
    "col_mask = df_cgy.reset_index().college==1\n",
    "ncol_mask = df_cgy.reset_index().college==0\n",
    "male_mask = df_cgy.reset_index().male==1\n",
    "female_mask = df_cgy.reset_index().male==0\n",
    "\n",
    "df_cy = pd.concat(\n",
    "    [fun(None),\n",
    "     fun(col_mask).rename(columns={'lnwkwage':'lnwkwage_col'}),\n",
    "     fun(ncol_mask).rename(columns={'lnwkwage':'lnwkwage_ncol'}),\n",
    "     fun(male_mask).rename(columns={'lnwkwage':'lnwkwage_male'}),\n",
    "     fun(female_mask).rename(columns={'lnwkwage':'lnwkwage_female'}),\n",
    "     fun(col_mask & male_mask).rename(columns={'lnwkwage':'lnwkwage_col_male'}),\n",
    "     fun(col_mask & female_mask).rename(columns={'lnwkwage':'lnwkwage_col_female'}),\n",
    "     fun(ncol_mask & male_mask).rename(columns={'lnwkwage':'lnwkwage_ncol_male'}),\n",
    "     fun(ncol_mask & female_mask).rename(columns={'lnwkwage':'lnwkwage_ncol_female'})\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CA shares\n",
    "share_cols = ['manuf_share', 'nonmanuf_share', 'unemp_share','nilf_share']\n",
    "def fun(m): \n",
    "    return WtMean(df_cgy.reset_index(), cols=share_cols, \n",
    "                  weight_col='weight', by_cols=['czone','year'], mask=m)\n",
    "col_mask = df_cgy.reset_index().college==1\n",
    "ncol_mask = df_cgy.reset_index().college==0\n",
    "\n",
    "df_cy = pd.concat(\n",
    "    [df_cy,\n",
    "     fun(None),\n",
    "     fun(col_mask).add_suffix('_col'),\n",
    "     fun(ncol_mask).add_suffix('_ncol'),\n",
    "    ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CA log counts\n",
    "# (We are taking a weighted average of logs. One could alternatively take the log of weighted averages)\n",
    "count_cols = ['lnmanuf','lnnonmanuf','lnemp','lnunemp','lnnilf','lnpop']\n",
    "df_cy = pd.concat([df_cy,\n",
    "                   WtMean(df_cgy.reset_index(), cols=count_cols, \n",
    "                          weight_col='weight', by_cols=['czone','year'])\n",
    "                  ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 10-year equivalent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_cy.columns.to_list()\n",
    "\n",
    "# Reshape to wide format:\n",
    "df_cy = df_cy.reset_index().pivot_table(index='czone', columns='year')\n",
    "\n",
    "# Compute decadal differences:\n",
    "for c in cols:\n",
    "    df_cy['D{}'.format(c),1990] = df_cy[c,2000] - df_cy[c,1990]\n",
    "    df_cy['D{}'.format(c),2000] = (df_cy[c,2008] - df_cy[c,2000])*(10/7)  \n",
    "    \n",
    "# Reshape back to long format:\n",
    "df_cy = df_cy.stack().drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name variables to be consistent with the ADH replication file and merge the explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in share_cols:\n",
    "    df_cy['D{}'.format(c)] = df_cy['D{}'.format(c)] * 100.0\n",
    "    df_cy['D{}_col'.format(c)] = df_cy['D{}_col'.format(c)] * 100.0\n",
    "    df_cy['D{}_ncol'.format(c)] = df_cy['D{}_ncol'.format(c)] * 100.0\n",
    "\n",
    "# Multiply by 100 b/c reports log points:\n",
    "cols_mask = df_cy.columns.str.contains('Dln')\n",
    "for c in df_cy.columns[cols_mask]:\n",
    "    df_cy[c] = df_cy[c] * 100.0\n",
    "    \n",
    "ADHnames = {\n",
    "    # outcome for Table 3\n",
    "    'Dmanuf_share' : 'd_sh_empl_mfg',\n",
    "\n",
    "    # outcomes for Table 5\n",
    "    # panel A\n",
    "    'Dlnmanuf' : 'lnchg_no_empl_mfg',\n",
    "    'Dlnnonmanuf' : 'lnchg_no_empl_nmfg',\n",
    "    'Dlnunemp' : 'lnchg_no_unempl',\n",
    "    'Dlnnilf' : 'lnchg_no_nilf',\n",
    "    # panel B\n",
    "    'Dmanuf_share' : 'd_sh_empl_mfg',\n",
    "    'Dnonmanuf_share' : 'd_sh_empl_nmfg',\n",
    "    'Dunemp_share' : 'd_sh_unempl',\n",
    "    'Dnilf_share' : 'd_sh_nilf',  \n",
    "    # panel C\n",
    "    'Dmanuf_share_col' : 'd_sh_empl_mfg_edu_c',\n",
    "    'Dnonmanuf_share_col' : 'd_sh_empl_nmfg_edu_c',\n",
    "    'Dunemp_share_col' : 'd_sh_unempl_edu_c',\n",
    "    'Dnilf_share_col' : 'd_sh_nilf_edu_c',\n",
    "    # panel D\n",
    "    'Dmanuf_share_ncol' : 'd_sh_empl_mfg_edu_nc',\n",
    "    'Dnonmanuf_share_ncol' : 'd_sh_empl_nmfg_edu_nc',\n",
    "    'Dunemp_share_ncol' : 'd_sh_unempl_edu_nc',\n",
    "    'Dnilf_share_ncol' : 'd_sh_nilf_edu_nc',\n",
    "    \n",
    "    # outcomes for Table 6\n",
    "    'Dlnwkwage' : 'd_avg_lnwkwage',\n",
    "    'Dlnwkwage_male' : 'd_avg_lnwkwage_m',\n",
    "    'Dlnwkwage_female' : 'd_avg_lnwkwage_f',\n",
    "    'Dlnwkwage_col' : 'd_avg_lnwkwage_c',\n",
    "    'Dlnwkwage_ncol' : 'd_avg_lnwkwage_nc',\n",
    "    'Dlnwkwage_col_male' : 'd_avg_lnwkwage_c_m',\n",
    "    'Dlnwkwage_col_female' : 'd_avg_lnwkwage_c_f',\n",
    "    'Dlnwkwage_ncol_male' : 'd_avg_lnwkwage_nc_m',\n",
    "    'Dlnwkwage_ncol_female' : 'd_avg_lnwkwage_nc_f'\n",
    "}\n",
    "\n",
    "df_cy.rename(columns=ADHnames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original non-CA data:\n",
    "df_NCA = pd.read_stata(mainp / 'files_provided/workfile_china.dta')\n",
    "\n",
    "# CA data:\n",
    "CA_cols = [v for k,v in ADHnames.items()]\n",
    "other_cols = df_NCA.columns.difference(CA_cols)\n",
    "df_CA = pd.merge(df_cy, df_NCA[other_cols], \n",
    "                 left_on=['czone','year'], right_on=['czone','yr'], how='inner')\n",
    "\n",
    "del df_cy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyIVreg(formula, df):\n",
    "    res = IV2SLS.from_formula(\n",
    "        formula,\n",
    "        df,\n",
    "        weights = df['timepwt48']\n",
    "    ).fit(cov_type=\"clustered\", clusters=df[\"statefip\"])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.latex.repr = True\n",
    "\n",
    "def CompareDF(x, fit_stats=['Estimator', 'R-squared', 'No. Observations'], keep=[]):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        y = pd.read_csv(StringIO(compare(x, stars=True, precision='std_errors').summary.as_csv()), \n",
    "                        skiprows=1, skipfooter=1, engine='python')\n",
    "    z = pd.DataFrame(\n",
    "        data=y.iloc[:, 1:].values,\n",
    "        index=y.iloc[:, 0].str.strip(),\n",
    "        columns=pd.MultiIndex.from_arrays(\n",
    "            arrays=[y.columns[1:], y.iloc[0][1:]],\n",
    "            names=['Model', 'Dep. Var.']\n",
    "        )\n",
    "    )\n",
    "    if not keep:\n",
    "        return pd.concat([z.iloc[11:], z.loc[fit_stats]])\n",
    "    else:\n",
    "        return pd.concat([*[z.iloc[z.index.get_loc(v):z.index.get_loc(v)+2] for v in keep], z.loc[fit_stats]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Table3(df):\n",
    "    regions = list(filter(lambda x: x.startswith(\"reg\"), df.columns))\n",
    "    controls = [\n",
    "        [\"t2\"],\n",
    "        [\"t2\",\"l_shind_manuf_cbp\"],\n",
    "        [\"t2\",\"l_shind_manuf_cbp\"] + regions,\n",
    "        [\"t2\",\"l_shind_manuf_cbp\", \"l_sh_popedu_c\", \"l_sh_popfborn\", \"l_sh_empl_f\"] + regions,\n",
    "        [\"t2\",\"l_shind_manuf_cbp\", \"l_task_outsource\", \"l_sh_routine33\"] + regions,\n",
    "        [\"t2\",\"l_shind_manuf_cbp\", \"l_sh_popedu_c\", \"l_sh_popfborn\", \"l_sh_empl_f\", \"l_task_outsource\", \"l_sh_routine33\"] + regions,\n",
    "    ]\n",
    "\n",
    "    baseform = \"d_sh_empl_mfg ~ [d_tradeusch_pw ~ d_tradeotch_pw_lag] + 1\"\n",
    "    models = {\n",
    "    '({})'.format(i+1) : ' + '.join([baseform, *controls[i]]) for i in range(len(controls))\n",
    "    }\n",
    "    res = {i : MyIVreg(m,df) for i, m in models.items()}\n",
    " \n",
    "    baseform_first = 'd_tradeusch_pw ~ d_tradeotch_pw_lag + 1'\n",
    "    models_first = {\n",
    "        '({})'.format(i+1) : ' + '.join([baseform_first, *controls[i]]) \n",
    "        for i in range(len(controls))\n",
    "    }\n",
    "    res_first = {i : MyIVreg(m,df) for i, m in models_first.items()}\n",
    "\n",
    "    return res, res_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Table5(df):\n",
    "    regions = list(filter(lambda x: x.startswith(\"reg\"), df.columns))\n",
    "    controls = ['t2','l_shind_manuf_cbp','l_sh_popedu_c','l_sh_popfborn','l_sh_empl_f','l_sh_routine33',\n",
    "                 'l_task_outsource'] + regions\n",
    "    lhs = {\n",
    "#         'A':['lnchg_no_empl_mfg','lnchg_no_empl_nmfg','lnchg_no_unempl','lnchg_no_nilf','lnchg_no_ssadiswkrs'],\n",
    "#         'B':['d_sh_empl_mfg','d_sh_empl_nmfg','d_sh_unempl','d_sh_nilf','d_sh_ssadiswkrs'],\n",
    "        'A':['lnchg_no_empl_mfg','lnchg_no_empl_nmfg','lnchg_no_unempl','lnchg_no_nilf'],\n",
    "        'B':['d_sh_empl_mfg','d_sh_empl_nmfg','d_sh_unempl','d_sh_nilf'],        \n",
    "        'C':['d_sh_empl_mfg_edu_c','d_sh_empl_nmfg_edu_c','d_sh_unempl_edu_c','d_sh_nilf_edu_c'],\n",
    "        'D':['d_sh_empl_mfg_edu_nc','d_sh_empl_nmfg_edu_nc','d_sh_unempl_edu_nc','d_sh_nilf_edu_nc']\n",
    "    }\n",
    "    models_a = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['A'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['A']))\n",
    "    }\n",
    "    models_b = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['B'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['B']))\n",
    "    }\n",
    "    models_c = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['C'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['C']))\n",
    "    }\n",
    "    models_d = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['D'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['D']))\n",
    "    }\n",
    "\n",
    "    res_a = {i : MyIVreg(m,df) for i, m in models_a.items()}\n",
    "    res_b = {i : MyIVreg(m,df) for i, m in models_b.items()}\n",
    "    res_c = {i : MyIVreg(m,df) for i, m in models_c.items()}\n",
    "    res_d = {i : MyIVreg(m,df) for i, m in models_d.items()}\n",
    "\n",
    "    return res_a, res_b, res_c, res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Table6(df):\n",
    "    regions = list(filter(lambda x: x.startswith(\"reg\"), df.columns))\n",
    "    controls = ['t2','l_shind_manuf_cbp','l_sh_popedu_c','l_sh_popfborn','l_sh_empl_f','l_sh_routine33',\n",
    "                 'l_task_outsource'] + regions\n",
    "    lhs = {\n",
    "        'A':['d_avg_lnwkwage','d_avg_lnwkwage_m','d_avg_lnwkwage_f'],\n",
    "        'B':['d_avg_lnwkwage_c','d_avg_lnwkwage_c_m','d_avg_lnwkwage_c_f'],\n",
    "        'C':['d_avg_lnwkwage_nc','d_avg_lnwkwage_nc_m','d_avg_lnwkwage_nc_f'],\n",
    "    }   \n",
    "    models_a = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['A'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['A']))\n",
    "    }\n",
    "    models_b = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['B'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['B']))\n",
    "    }\n",
    "    models_c = {\n",
    "        '({})'.format(i+1) : ' + '.join(['{} ~ 1 + [d_tradeusch_pw ~ d_tradeotch_pw_lag]'.format(lhs['C'][i]), \n",
    "                                         *controls]) for i in range(len(lhs['C']))\n",
    "    }\n",
    "    res_a = {i : MyIVreg(m,df) for i, m in models_a.items()}\n",
    "    res_b = {i : MyIVreg(m,df) for i, m in models_b.items()}\n",
    "    res_c = {i : MyIVreg(m,df) for i, m in models_c.items()}\n",
    "    \n",
    "    return res_a, res_b, res_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3: Change in Manuf/Pop, Pooled Regressions with Controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. 1990–2007 stacked first differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['d_tradeusch_pw','l_shind_manuf_cbp', 'l_sh_popedu_c', 'l_sh_popfborn', 'l_sh_empl_f', \n",
    "        'l_task_outsource', 'l_sh_routine33']\n",
    "CompareDF(Table3(df_CA)[0], keep = keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**. In Column 1 we are estimating \n",
    "$$ 100 \\times \\Delta L^m_{it} = \\alpha + \\beta \\Delta IPW_{uit} + \\gamma_t + e_{it} $$\n",
    "where $L^m_{it}$ is (manufacturing employment)/(working-age population) and  $IPW_{uit}$ is the import exposure per worker measured in 1,000s of dollars (see Appendix Table 1 of ADH). Then an estimate $\\widehat{\\beta}=-0.7871$ means that an exogenous increase of $1,000 in exposure per worker leads to a predicted decrease of 0.79 percentage points in manufacturing employment per working-age population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2SLS by Frisch-Waugh-Lovell - Column 3 of Table 3\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Residualize on controls:\n",
    "regions = list(filter(lambda x: x.startswith(\"reg\"), df_CA.columns))\n",
    "controls = [\"t2\",\"l_shind_manuf_cbp\"] + regions\n",
    "W = sm.add_constant(df_CA[controls])\n",
    "r_x = sm.WLS(df_CA['d_tradeusch_pw'], W, weights = df_CA['timepwt48']).fit().resid\n",
    "r_y = sm.WLS(df_CA['d_sh_empl_mfg'], W, weights = df_CA['timepwt48']).fit().resid\n",
    "r_z = sm.WLS(df_CA['d_tradeotch_pw_lag'], W, weights = df_CA['timepwt48']).fit().resid\n",
    "\n",
    "# Predict X with Z:\n",
    "x_hat = sm.WLS(r_x, r_z, weights = df_CA['timepwt48']).fit().predict()\n",
    "\n",
    "# Regress Y on predicted X:\n",
    "sm.WLS(r_y, x_hat, weights = df_CA['timepwt48']).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. 2SLS first stage estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(Table3(df_CA)[1], keep=['d_tradeotch_pw_lag'], fit_stats=['R-squared'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 5: Change in Employment, Unemployment and Non-Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5a, results5b, results5c, results5d = Table5(df_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel A. 100 × log change in population counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results5a, keep=['d_tradeusch_pw'], fit_stats=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel B. Change in population shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results5b, keep=['d_tradeusch_pw'], fit_stats=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### College education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results5c, keep=['d_tradeusch_pw'], fit_stats=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No college education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results5d, keep=['d_tradeusch_pw'], fit_stats=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 6: Wage Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6a, results6b, results6c = Table6(df_CA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel A. All education levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results6a, keep=['d_tradeusch_pw'], fit_stats=['R-squared'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel B. College education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results6b, keep=['d_tradeusch_pw'], fit_stats=['R-squared'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel C. No college education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompareDF(results6c, keep=['d_tradeusch_pw'], fit_stats=['R-squared'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
